#!/usr/bin/env python3
"""
The automatic database generation for TensorAlloy.
"""
import numpy as np
import pandas as pd
import toml
import os
import json
import hashlib
from dataclasses import dataclass
from pathlib import Path
from ase.io import read, write
from ase import Atoms
from ase.build import bulk
from ase.calculators.vasp import Vasp
from subprocess import PIPE, Popen
from argparse import ArgumentParser
from collections import Counter
from tensoralloy.io.vasp import read_vasp_xml


@dataclass(frozen=True)
class ServiceUnit:
    """ 
    The data structure for estimating VASP costs.
    
    Attributes
    ----------
    device : str
        The device type, cpu or gpu.
    n : int
        The number of CPUs or GPUs used for this task.
    t : float
        The walltime in seconds for this task.
    hours : float
        The total deivce hours.

    """
    device: str = "cpu"
    n: int = 0
    elapsed: float = 0.0
    hours: float = 0.0


def getitem(obj, keys):
    """
    Get the item from the nested dictionary.
    """
    for key in keys:
        obj = obj.get(key, {})
    return obj


def scalar2array(obj, size, n1=None, n2=None):
    """
    Convert the scalar to an array.
    """
    if hasattr(obj, "__len__"):
        if len(obj) != size:
            if n1 is not None and n2 is not None:
                msg = f"The length of {n1} is not equal to {n2}"
            else:
                msg = f"The length of obj is not equal to {size}"
            raise ValueError(msg)
        return np.asarray(obj)
    else:
        return np.asarray([obj] * size)


def get_vasp_elapsed_time(outcar):
    """
    Get the elapsed time (seconds) of a VASP job given its OUTCAR.
    """
    cmd = f"tail -n 10 {str(outcar)} | grep Elapsed"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    stdout, _ = p.communicate()
    if p.returncode != 0:
        return -1.0
    return float(stdout.decode().split()[3])


def get_vasp_mpi_omp_ranks(outcar):
    """
    Get the number of mpi and openmp ranks of a VASP job given its OUTCAR.
    """
    cmd = f"head -n 10 {str(outcar)} | grep mpi-ranks"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    stdout, _ = p.communicate()
    if p.returncode == 0:
        values = stdout.decode().split()
        mpi = int(values[1])
        omp = int(values[4])
        return mpi, omp
    else:
        return 0, 0


def get_vasp_running_device(outcar):
    """
    Return the deivce (cpu or gpu) used to run the VASP job.
    """
    cmd = f"head -n 10 {str(outcar)} | grep GPUs"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    stdout, _ = p.communicate()
    if p.returncode == 0:
        return "gpu"
    else:
        return "cpu"


def get_vasp_job_service_unit(outcar) -> ServiceUnit:
    """
    Get the service unit (SU) of a completed VASP job.
    """
    elapsed = get_vasp_elapsed_time(outcar)
    if elapsed < 0:
        return None
    mpi, omp = get_vasp_mpi_omp_ranks(outcar)
    device = get_vasp_running_device(outcar)
    if device == "gpu":
        hours = mpi * elapsed / 3600.0
    else:
        hours = mpi * omp * elapsed / 3600.0
    return ServiceUnit(device, mpi, elapsed, hours)


class TensorDB:
    """
    The database generation kit for TensorAlloy.
    """

    def __init__(self, input_file):
        """
        Initialization method.
        """
        self.input_file = input_file
        self.config = dict(toml.load(input_file))
        self.species = self.config["species"]
        self.phases = self.config["phases"]
        self.base_phase_structures = self.init_phases()
        self.init_vasp()

    def init_phases(self):
        """
        Initialize the base phase structures.
        """
        workdir = Path("structures")
        structures = {}
        for phase in self.config["phases"]:
            if phase == "liquid":
                structures[phase] = self.init_liquid_structure()
            else:
                candidates = [x for x in workdir.glob(f"{phase}.*")]
                if len(candidates) == 0:
                    raise ValueError(f"Cannot find the poscar for {phase}")
                if len(candidates) > 1:
                    raise ValueError(f"Multiple poscars for {phase}")
                poscar = candidates[0]
                structures[phase] = read(str(poscar))
        return structures

    def init_liquid_structure(self):
        """
        Initialize the liquid phase structure.

        Num species = 1: use the fcc phase.
        Num species > 1: use SAE (todo)
        
        """
        if len(self.species) == 1:
            veq = self.config["liquid"]["veq"]
            a = (4 * veq)**(1 / 3)
            return bulk(self.species[0], crystalstructure="fcc", a=a, 
                        cubic=True)
        else:
            raise NotImplementedError(
                "Liquid phase for multi-species is not implemented yet.")

    def get_base_structure(self, phase: str) -> Atoms:
        """
        Get the base phase structure.
        """
        return self.base_phase_structures[phase].copy()
    
    def get_supercells_at_volume(self, phase: str, volume: float) -> Atoms:
        """
        Get the supercell structure at the given atomic volume.
        """
        base = self.get_base_structure(phase)
        scale = (volume / base.get_volume() * len(base))**(1 / 3)
        base.set_cell(base.get_cell() * scale, scale_atoms=True)
        supercells = []
        for replicate in self.config[phase]["supercell"]:
            supercells.append(base * replicate)
        return supercells
    
    def init_vasp(self):
        """
        Initialize the base VASP calculator.
        """
        params = getitem(self.config, ["vasp"])
        
        # Setup the POTCARs path
        os.environ["VASP_PP_PATH"] = params["pp_path"]

        # Setup PORCAR for each specie. 
        # For ASE VASP calculator, only the suffix is used.
        setups = {'base': 'recommended'}
        if "potcars" in params:
            for i, potcar in enumerate(params["potcars"]):
                if potcar != self.species[i]:
                    setups[potcar] = potcar[len(self.species[i]):]

        # Initialize the VASP calculator for AIMD sampling jobs.
        self.vasp = Vasp(
            xc=params.get("xc", "pbe"),
            setups=setups, 
            ediff=params.get("ediff", 1e-5),
            lreal=params.get("lreal", "Auto"), 
            prec=params.get("prec", "Normal"),
            encut=params.get("encut", 400),
            ismear=params.get("ismear", 1),
            sigma=params.get("sigma", 0.1),
            algo=params.get("algo", "normal"),
            isym=params.get("isym", 0),
            nelmin=params.get("nelmin", 4),
            isif=params.get("isif", 2),
            kpar=params.get("kpar", 1),
            npar=params.get("npar", 8),
            ibrion=params.get("ibrion", 0),
            nsw=params.get("nsw", 10000),
            potim=params.get("potim", 1),
            nwrite=params.get("nwrite", 1),
            lcharg=params.get("lcharg", False),
            lwave=params.get("lwave", False),
            nblock=params.get("nblock", 1),
            maxmix=params.get("maxmix", 60))  
        
        # Initialize the VASP calculator for high-precision DFT calculations.
        self.prec_vasp = Vasp(
            xc=params.get("xc", "pbe"),
            setups=setups,
            ediff=params.get("ediff", 1e-6),
            lreal=params.get("lreal", "False"),
            prec=params.get("prec", "Accurate"),
            encut=params.get("encut", 500),
            ismear=params.get("ismear", 1),
            sigma=params.get("sigma", 0.05),
            algo=params.get("algo", "normal"),
            isym=params.get("isym", 0),
            nelmin=params.get("nelmin", 4),
            isif=params.get("isif", 2),
            ibrion=params.get("ibrion", -1),
            nsw=params.get("nsw", 1),
            potim=params.get("potim", 0),
            nwrite=params.get("nwrite", 1),
            lcharg=params.get("lcharg", False),
            lwave=params.get("lwave", False),
            nblock=params.get("nblock", 1),
            maxmix=params.get("maxmix", 60))
    
    # --------------------------------------------------------------------------
    # Iterators
    # --------------------------------------------------------------------------

    def sampling_task_iterator(self):
        """
        Iterate through all AIMD sampling job dirs. 
        """
        workdir = Path("sampling")
        return workdir.glob("*/n[pv]t/*/?_*K_to_*K")   

    def accurate_dft_calc_iterator(self):
        """
        Iterate through all high precision dft calculation job dirs.
        """
        workdir = Path("calc")
        return workdir.glob("*atoms/group*/task*")

    # --------------------------------------------------------------------------
    # Generate VASP AIMD sampling jobs
    # --------------------------------------------------------------------------

    def setup_vasp_sampling_parameters(self, npt=False):
        """
        Setup the sampling parameters for VASP.
        """
        params = getitem(self.config, ["vasp", "sampling"])
        gamma = params.get("langevin_gamma", 10)
        ismear = params.get("ismear", 0)
        sigma = params.get("sigma", 0.1)
        npar = params.get("npar", 8)
        encut = params.get("encut", 350)
        ediff = params.get("ediff", 1e-5)

        # MDALGO = 3 for Langevin thermostat
        # ISIF = 2 for NVT ensemble
        # LANGEVIN_GAMMA: Langevin friction coefficient for atom species
        self.vasp.set(mdalgo=3, langevin_gamma=[gamma] * len(self.species))
        self.vasp.set(ismear=ismear, sigma=sigma, npar=npar, kpar=1,
                      encut=encut, ediff=ediff)
        
        # Special parameters for Parrinello-Rahman NPT. 
        # ISIF = 3 for NPT ensemble. 
        # LANGEVIN_GAMMA_L: Langevin friction coefficient for lattice degrees of 
        # freedom.
        # PMASS (optional tag for VASP) is ignored.
        if npt:
            npt_params = getitem(params, ["npt"])
            gamma = npt_params.get("langevin_gamma_l", 10)
            self.vasp.set(isif=3, langevin_gamma_l=gamma)

    @staticmethod
    def _get_nvt_sampling_task(jobdir: Path, t0: float, t1: float, 
                               natoms: int, override=False):
        """
        Returns (id, name) for the NVT sampling task. 
        If the returned id is -1, the task should be skipped.
        """
        files = [x.name 
                for x in jobdir.glob(f"*_{natoms}atoms_*K_to_*K")]
        key = f"{natoms}atoms_{t0:.0f}K_to_{t1:.0f}K"
        exists = False
        for afile in files:
            if afile.endswith(key):
                exists = True
                break
        if exists and not override:
            return -1, None
        if override:
            newid = len(files)
        else:
            newid = len(files) + 1
        return newid, f"{newid}_{key}"
    
    def create_vasp_sampling_nvt_tasks(self, override=False):
        """
        Create VASP Langeven NVT sampling jobs: gamma-only.
        """
        workdir = Path("sampling")
        workdir.mkdir(exist_ok=True)

        # The NVT vasp parameters
        self.setup_vasp_sampling_parameters(npt=False)

        for phase in self.phases:
            # The phase sampling parameters
            args = getitem(self.config, ["vasp", "sampling", "nvt", phase])
            if len(args) == 0:
                continue
            
            # Get the volumes and temperatures
            volumes = args.get("volumes", [])
            if len(volumes) == 0:
                continue
            size = len(volumes)
            n2 = "volumes"

            # Get initial & final temperatures and steps
            t0 = scalar2array(args["tstart"], size, "tstart", n2)
            t1 = scalar2array(args["tstop"], size, "tstop", n2)
            steps = scalar2array(args["nsteps"], size, "nsteps", n2).astype(int)

            # Loop through the volumes
            for i, vol in enumerate(volumes):
                jobdir = workdir / phase / f"nvt/v{np.round(vol*100, 0):.0f}"
                jobdir.mkdir(parents=True, exist_ok=True)

                # Get the supercell structures
                supercells = self.get_supercells_at_volume(phase, vol)
                if len(supercells) == 0:
                    continue

                for supercell in supercells:
                    # Determine the task id and name
                    taskid, taskname = self._get_nvt_sampling_task(
                        jobdir, t0[i], t1[i], len(supercell), override)
                    if taskid < 0:
                        continue
                    # setup the Vasp calculator and generate input files.
                    taskdir = jobdir / taskname
                    taskdir.mkdir(exist_ok=True)
                    self.vasp.set(directory=str(taskdir))
                    self.vasp.set(tebeg=t0[i], teend=t1[i], nsw=steps[i])
                    self.vasp.write_input(supercell)
                    metadata = {
                        "phase": str(phase), "ensemble": "nvt",
                        "P/V": f"v{vol:.2f}", "Tstart": int(t0[i]), 
                        "Tstop": int(t1[i]), "nsw": int(steps[i])
                    }
                    with open(taskdir / "metadata.json", "w") as fp:
                        json.dump(metadata, fp, indent=2)
                        fp.write("\n")
                    print(f"[VASP/nvt/sampling/gen]: {taskdir}")

    @staticmethod
    def _get_npt_sampling_task(jobdir: Path, t0: float, t1: float, v0: float,
                               natoms: int, override=False):
        """
        Returns (id, name) for the NVT sampling task. 
        If the returned id is -1, the task should be skipped.
        """
        vkey = f"v{np.round(v0*100, 0):.0f}"
        files = [x.name 
                 for x in jobdir.glob(f"*_{natoms}atoms_{vkey}_*K_to_*K")]
        key = f"{natoms}atoms_{vkey}_{t0:.0f}K_to_{t1:.0f}K"
        exists = False
        for afile in files:
            if afile.endswith(key):
                exists = True
                break
        if exists and not override:
            return -1, None
        if override:
            newid = len(files)
        else:
            newid = len(files) + 1
        return newid, f"{newid}_{key}"

    def create_vasp_sampling_npt_tasks(self, override=False):
        """
        Create VASP Parrinello-Rahman NPT sampling jobs
        """
        workdir = Path("sampling")
        workdir.mkdir(exist_ok=True)

        # The NPT vasp parameters
        self.setup_vasp_sampling_parameters(npt=True)

        for phase in self.phases:
            # The phase sampling parameters
            args = getitem(self.config, ["vasp", "sampling", "npt", phase])
            if len(args) == 0:
                continue
            
            # Get desired pressures
            pressures = args.get("pressures", [])
            if len(pressures) == 0:
                continue
            size = len(pressures)

            # Get initial volumes, intial & final temperatures and steps
            n2 = "pressures"
            volumes = scalar2array(args["volumes"], size, "volumes", n2)
            t0 = scalar2array(args["tstart"], size, "tstart", n2)
            t1 = scalar2array(args["tstop"], size, "tstop", n2)
            steps = scalar2array(args["nsteps"], size, "nsteps", n2).astype(int)
            
            # Loop through the pressures
            for i, pressure in enumerate(pressures):
                jobdir = workdir / phase / f"npt/{pressure:.0f}GPa"
                jobdir.mkdir(parents=True, exist_ok=True)

                # Get the supercell structures
                supercells = self.get_supercells_at_volume(phase, volumes[i])
                if len(supercells) == 0:
                    continue

                for supercell in supercells:
                    # Determine the task id and name
                    taskid, taskname = self._get_npt_sampling_task(
                        jobdir, t0[i], t1[i], volumes[i], len(supercell), 
                        override)
                    if taskid < 0:
                        continue
                    # setup the Vasp calculator and generate input files.
                    taskdir = jobdir / taskname
                    taskdir.mkdir(exist_ok=True)
                    self.vasp.set(directory=str(taskdir))
                    self.vasp.set(tebeg=t0[i], teend=t1[i], nsw=steps[i])
                    self.vasp.set(pstress=pressure * 10)
                    self.vasp.write_input(supercell)
                    metadata = {
                        "phase": str(phase), "ensemble": "npt", 
                        "P/V": f"{pressure}GPa", "Tstart": int(t0[i]), 
                        "Tstop": int(t1[i]), "nsw": int(steps[i])
                    }
                    with open(taskdir / "metadata.json", "w") as fp:
                        json.dump(metadata, fp, indent=2)
                        fp.write("\n")
                    print(f"[VASP/npt/sampling/gen]: {taskdir}")
    
    # --------------------------------------------------------------------------
    # Check the status of sampling jobs
    # --------------------------------------------------------------------------

    @staticmethod
    def _is_sampling_job_finished(jobdir: Path):
        """
        Return True if the sampling job is finished.
        """
        metadata = jobdir / "metadata.json"
        if not metadata.exists():
            return False
        with open(metadata, "r") as fp:
            status = json.load(fp)
        if status.get("elapsed (h)", -1) <= 0:
            return False
        return True
        
    def update_status_of_sampling_job(self, jobdir: Path):
        """
        Update and return the status of a sampling job at given jobdir.
        """
        metadata = jobdir / "metadata.json"
        if not metadata.exists():
            return {}
        
        with open(metadata, "r") as fp:
            status = json.load(fp)
                
        status["nrun"] = -1
        status["device"] = ""
        status["SU"] = -1
        status["processed"] = "n"
        
        oszicar = jobdir / "OSZICAR"
        if not oszicar.exists():
            return status
        
        cmd = f"grep '.*T=.*E=.*' {oszicar} | tail -n 1"
        p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
        stdout, _ = p.communicate()        
        if p.returncode != 0:
            return status
        
        nrun = int(stdout.decode().split()[0])
        status["nrun"] = nrun

        # The simulation is not finished
        if nrun < status["nsw"]:
            return status

        # The simulation is finished
        su = get_vasp_job_service_unit(jobdir / "OUTCAR")
        status["device"] = su.device
        status["SU"] = np.round(su.hours, 2)
        with open(metadata, "w") as fp:
            json.dump(status, fp, indent=2)
            fp.write("\n")
        
        # The job is already postprocessed.
        output_file = jobdir / "trajectory.extxyz"
        if output_file.exists():
            status["processed"] = "y"

        return status

    def get_status_of_all_sampling_jobs(self):
        """ 
        Get the status of all sampling jobs.
        """
        status = {}
        for task in self.sampling_task_iterator():
            info = self.update_status_of_sampling_job(task)
            if len(info) == 0:
                continue
            for key, value in info.items():
                status[key] = status.get(key, []) + [value, ]
        df = pd.DataFrame(status)
        df.set_index("phase", inplace=True)
        print(df.to_string())

    # --------------------------------------------------------------------------
    # Post-processing
    # --------------------------------------------------------------------------
    
    def post_process_sampling_job(self, jobdir: Path):
        """
        Post-processing a sampling job.
        """
        # Check if the job is finished
        if not self._is_sampling_job_finished(jobdir):
            return
        # Check if the vasprun.xml file exists
        vasprun_xml = jobdir / "vasprun.xml"
        if not vasprun_xml.exists():
            return
        # Check if the trajectory file exists
        output_file = jobdir / "trajectory.extxyz"
        if output_file.exists():
            return
        # Read the xml file using the api `tensoralloy.io.vasp.read_vasp_xml`
        try:
            trajectory = [
                atoms for atoms in read_vasp_xml(vasprun_xml, 
                                                 index=slice(0, None, 1), 
                                                 finite_temperature=False)]
        except Exception:
            return
        if len(trajectory) == 0:
            return
        # Add the source information to the atoms.info
        for i, atoms in enumerate(trajectory):
            src = f"{str(jobdir)}@{i}"
            atoms.info['_source'] = src
            atoms.info['_hash'] = hashlib.md5(src.encode()).hexdigest()
        # Save the trajectory to an extxyz file
        write(output_file, trajectory, format="extxyz")
        print(f"[VASP/sampling/postprocess]: {jobdir}")
    
    def post_process_all_sampling_jobs(self):
        """
        Post-processing all sampling jobs.
        """
        for task in self.sampling_task_iterator():
            self.post_process_sampling_job(task)
    
    # --------------------------------------------------------------------------
    # High-precision DFT calculations
    # --------------------------------------------------------------------------
            
    def setup_vasp_accurate_dft_parameters(self, atoms):
        """
        Setup the parameters for high-precision DFT calculations.

        ASE doe not implement the non-collinear settings, so we should hack it.
        """
        params = getitem(self.config, ["vasp", "calc"])
        magmon_orig_value = None
        for key, value in params.items():
            if key == "magmom":
                magmon_orig_value = value
            else:
                self.prec_vasp.set(**{key: value})
        if magmon_orig_value is not None:
            if self.prec_vasp.bool_params['lsorbit']:
                magmom = f"{len(atoms)*3}*{magmon_orig_value}"
            else:
                magmom = f"{len(atoms)}*{magmon_orig_value}"
        else:
            magmom = None
        return {"MAGMOM": magmom}

    def create_vasp_accurate_dft_tasks(self, interval=50, shuffle=False):
        """
        Create VASP high precision DFT calculation tasks.
        """
        workdir = Path("calc")
        workdir.mkdir(exist_ok=True)

        # The global hash table
        hash_file = workdir / "hash.json"
        
        # The global training structures file
        calc_file = workdir / "accurate_dft_calc.extxyz"
        
        # May read existing results
        if hash_file.exists():
            with open(hash_file, "r") as fp:
                hash_table = json.load(fp)
            calc_list = read(calc_file, index=":")
            if len(calc_list) != len(hash_table):
                raise IOError(
                    f"{calc_file}(n={len(calc_list)}) does not "
                    f"match with {hash_file}(n={len(hash_table)})!")
        else:
            hash_table = {}
            calc_list = []    
        
        # Initialize the subsets
        subset_id = Counter()
        for atoms in calc_list:
            subset_id[len(atoms)] += 1

        # Loop through all sampling tasks
        for task in self.sampling_task_iterator():
            
            # Check if the trajectory file exists
            trajectory = task / "trajectory.extxyz"
            if not trajectory.exists():
                continue
            # Read the trajectory file
            full = read(trajectory, index=slice(0, None, 1))
            if shuffle:
                size = len(full) // interval
                selected = np.random.choice(full[interval-1:], size=size, 
                                            replace=False)
            else:
                # Select the last snapshot of each block(size=interval)
                selected = full[interval-1::interval]
            for atoms in selected:
                hash_id = atoms.info["_hash"]
                src = atoms.info["_source"]
                if hash_id in hash_table:
                    continue
                else:
                    calc_list.append(atoms)
                    natoms = len(atoms)
                    aid = f"{natoms}.{subset_id[natoms]}"
                    hash_table[hash_id] = {"aid": aid, "source": src}
                    subset_id[len(atoms)] += 1
       
        # Save the hash table
        with open(hash_file, "w") as fp:
            json.dump(hash_table, fp, indent=2)
            fp.write("\n")
        
        # Save the structures
        write(calc_file, calc_list, format="extxyz")

        # Create VASP jobs
        subset_size = Counter()
        for atoms in calc_list:

            # The original structure id
            aid = hash_table[atoms.info["_hash"]]["aid"]

            # For structures of different sizes, we may use different CPU/GPU
            # settings. Hence, 'natoms' is the first metric for makeing subsets.
            natoms = len(atoms)
            subsetdir = workdir / f"{natoms}atoms"
            subsetdir.mkdir(exist_ok=True)
            if natoms not in subset_size:
                subset_size[natoms] = Counter()

            # The group id. Each group contains 100 structures at most.
            sid = int(aid.split(".")[1])
            group_id = sid // 100 
            groupdir = subsetdir / f"group{group_id}"
            groupdir.mkdir(exist_ok=True)

            # The task id.
            task_id = sid % 100
            taskdir = groupdir / f"task{task_id}"
            taskdir.mkdir(exist_ok=True)

            # Setup the VASP calculator
            magmom_dct = self.setup_vasp_accurate_dft_parameters(atoms)

            # Write the input files
            self.prec_vasp.set(directory=str(taskdir))
            self.prec_vasp.write_input(atoms)
            
            # Hack the MAGMOM tag for non-collinear calculations
            if magmom_dct is not None:
                with open(taskdir / "INCAR", "a") as fp:
                    fp.write("\n")
                    for key, value in magmom_dct.items():
                        fp.write(f" {key} = {value}\n")

            subset_size[natoms][group_id] += 1
            
            # Write the metadata
            metadata = {
                "source": atoms.info["_source"],
                "hash": atoms.info["_hash"],
                "aid": aid,
                "group_id": group_id,
                "task_id": task_id}
            with open(taskdir / "metadata.json", "w") as fp:
                json.dump(metadata, fp, indent=2)
                fp.write("\n")
        
        for natoms, group_size in subset_size.items():
            for group_id, size in group_size.items():
                print(f"[VASP/calc/create/{natoms}]: "
                      f"group{group_id} ({size} tasks)")
    
    def get_accurate_dft_calculation_status(self):
        """
        Get the status of high precision dft calculations.
        """

        # Determine the total number of prepared jobs
        hash_file = Path("calc/hash.json")
        if not hash_file.exists():
            return
        with open(hash_file) as fp:
            hash_table = json.load(fp)
        subset_size = Counter()
        for key, value in hash_table.items():
            aid = value["aid"]
            i, j = [int(x) for x in aid.split(".")]
            subset_size[i] = max(subset_size[i], j + 1)
        
        status = {"group": [], "jobs": [], "completed": [], 
                  "CPU*hours": [], "GPU*hours": []}
        accumulator = {}

        # Loop through all prepared jobs
        for taskdir in self.accurate_dft_calc_iterator():
            metadata = taskdir / "metadata.json"
            if not metadata.exists():
                continue
            with open(metadata, "r") as fp:
                metadata = json.load(fp)
            sid, aid = [int(x) for x in metadata["aid"].split(".")]
            gid = metadata["group_id"]
            key = (sid, gid)
            if key not in accumulator:
                if (gid + 1) * 100 <= subset_size[sid]:
                    jobs = 100
                else:
                    jobs = aid % 100
                accumulator[key] = Counter({
                    'CPU*hours': 0.0, 'GPU*hours': 0.0, 
                    'completed': 0, 'jobs': jobs})
            su = get_vasp_job_service_unit(taskdir / 'OUTCAR')
            if su is None:
                continue
            accumulator[key]['completed'] += 1
            if su.device == "cpu":
                accumulator[key]['CPU*hours'] += su.hours
            else:
                accumulator[key]['GPU*hours'] += su.hours
        
        # Print the status
        for key, value in accumulator.items():
            status["group"].append(f"{key[0]}.g{key[1]}")
            status["jobs"].append(value["jobs"])
            status["completed"].append(value["completed"])
            status["CPU*hours"].append(np.round(value["CPU*hours"], 2))
            status["GPU*hours"].append(np.round(value["GPU*hours"], 2))
        status["group"].append("overall")
        status["jobs"].append(sum(status["jobs"]))
        status["completed"].append(sum(status["completed"]))
        status["CPU*hours"].append(sum(status["CPU*hours"]))
        status["GPU*hours"].append(sum(status["GPU*hours"]))
        df = pd.DataFrame(status)
        df.set_index("group", inplace=True)
        print(df.to_string())
    
    # --------------------------------------------------------------------------
    # List jobs that are not submitted
    # --------------------------------------------------------------------------
    


def main():
    parser = ArgumentParser()
    parser.add_argument("input", help="The input file")
    parser.add_argument("--status", choices=["sampling", "calc"], 
                        default=None, help="Get the status of tasks.")
    parser.add_argument("--nvt", action="store_true", default=False, 
                        help="Generate NVT sampling tasks")
    parser.add_argument("--npt", action="store_true", default=False,
                        help="Generate NPT sampling tasks")
    parser.add_argument("--override", action="store_true", default=False, 
                        help="Override the existing tasks")
    parser.add_argument("--postprocess", action="store_true", default=False,
                        help="Post-process the sampling jobs")
    parser.add_argument("--create", action="store_true", default=False,
                        help="Create the high-precision DFT calculation tasks")
    parser.add_argument("--interval", type=int, default=50)
    args = parser.parse_args()
    kit = TensorDB(args.input)

    if args.status is not None:
        if args.status == "sampling":
            kit.get_status_of_all_sampling_jobs()
        else:
            kit.get_accurate_dft_calculation_status()
    elif args.postprocess:
        kit.post_process_all_sampling_jobs()
    elif args.create:
        kit.create_vasp_accurate_dft_tasks(interval=args.interval)
    else:
        if args.nvt:
            kit.create_vasp_sampling_nvt_tasks(override=args.override)
        if args.npt:
            kit.create_vasp_sampling_npt_tasks(override=args.override)


if __name__ == "__main__":
    main()
