#!/usr/bin/env python3
"""
The automatic database generation for TensorAlloy.
"""
import numpy as np
import pandas as pd
import toml
import os
import json
import tqdm
import hashlib
import matplotlib
import re
from datetime import datetime
from dataclasses import dataclass
from pathlib import Path
from ase.io import read, write
from ase import Atoms
from ase.build import bulk
from ase.calculators.vasp import Vasp
from ase.units import GPa, kB
from ase.neighborlist import neighbor_list
from subprocess import PIPE, Popen
from argparse import ArgumentParser
from collections import Counter
from tensoralloy.io.vasp import read_vasp_xml
from matplotlib import pyplot as plt


@dataclass(frozen=True)
class ServiceUnit:
    """
    The data structure for estimating VASP costs.

    Attributes
    ----------
    device : str
        The device type, cpu or gpu.
    n : int
        The number of CPUs or GPUs used for this task.
    t : float
        The walltime in seconds for this task.
    hours : float
        The total deivce hours.

    """

    device: str = "cpu"
    n: int = 0
    elapsed: float = 0.0
    hours: float = 0.0


def getitem(obj, keys):
    """
    Get the item from the nested dictionary.
    """
    for key in keys:
        obj = obj.get(key, {})
    return obj


def scalar2array(obj, size, n1=None, n2=None):
    """
    Convert the scalar to an array.
    """
    if hasattr(obj, "__len__"):
        if len(obj) != size:
            if len(obj) == 1:
                return np.asarray([obj[0]] * size)
            if n1 is not None and n2 is not None:
                msg = f"The length of {n1} is not equal to {n2}"
            else:
                msg = f"The length of obj is not equal to {size}"
            raise ValueError(msg)
        return np.asarray(obj)
    else:
        return np.asarray([obj] * size)


def get_vasp_elapsed_time(outcar):
    """
    Get the elapsed time (seconds) of a VASP job given its OUTCAR.
    """
    cmd = f"tail -n 10 {str(outcar)} | grep Elapsed"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    stdout, _ = p.communicate()
    if p.returncode != 0:
        return -1.0
    return float(stdout.decode().split()[3])


def get_vasp_mpi_omp_ranks(outcar):
    """
    Get the number of mpi and openmp ranks of a VASP job given its OUTCAR.
    """
    cmd = f"head -n 10 {str(outcar)} | grep mpi-ranks"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    stdout, _ = p.communicate()
    if p.returncode == 0:
        values = stdout.decode().split()
        mpi = int(values[1])
        omp = int(values[4])
        return mpi, omp
    else:
        return 0, 0


def get_vasp_running_device(outcar):
    """
    Return the deivce (cpu or gpu) used to run the VASP job.
    """
    cmd = f"head -n 10 {str(outcar)} | grep GPUs"
    p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
    p.communicate()
    if p.returncode == 0:
        return "gpu"
    else:
        return "cpu"


def get_vasp_job_service_unit(outcar) -> ServiceUnit:
    """
    Get the service unit (SU) of a completed VASP job.
    """
    if not outcar.exists():
        return None
    elapsed = get_vasp_elapsed_time(outcar)
    if elapsed < 0:
        return None
    mpi, omp = get_vasp_mpi_omp_ranks(outcar)
    device = get_vasp_running_device(outcar)
    if device == "gpu":
        hours = mpi * elapsed / 3600.0
    else:
        hours = mpi * omp * elapsed / 3600.0
    return ServiceUnit(device, mpi, elapsed, hours)


def check_vasp_job_scf_convergence(outcar):
    """
    Check the SCF convergence of a VASP job.
    """
    outcar = Path(outcar)
    oszicar = outcar.parent / "OSZICAR"
    if not oszicar.exists():
        return False
    ediff_patt = re.compile(
        r"\s+EDIFF\s+=\s+([0-9\-+E.]+)\s+stopping-criterion.*")
    scf_patt = re.compile(r"(DAV|RMM):\s+(\d+)\s+([0-9\-+E.]+)\s+([0-9\-+E.]+)\s+(.*)")
    ediff = 1e-6
    with open(outcar) as fp:
        for ln, line in enumerate(fp):
            m = ediff_patt.search(line)
            if m:
                ediff = float(m.group(1))
                break
            if ln > 2000:
                return False
    dE = ediff * 10.0
    with open(oszicar) as fp:
        for line in fp:
            m = scf_patt.search(line)
            if m:
                dE = abs(float(m.group(4)))
    if dE <= ediff:
        return True
    else:
        return False


def asarray_or_eval(array):
    """
    Convert the 'array' to an array by eval if it is a string.
    """
    if isinstance(array, str):
        return eval(array)
    else:
        z = np.asarray(array)
        if z.ndim == 0:
            return np.atleast_1d(z)
        else:
            return z


class TensorDB:
    """
    The database generation kit for TensorAlloy.
    """

    def __init__(self, input_file):
        """
        Initialization method.
        """
        self.input_file = input_file
        self.root = Path(input_file).parent
        self.config = dict(toml.load(input_file))
        if self.config.get("finite_temperature", False):
            self.is_finite_temperature = True
        else:
            self.is_finite_temperature = False
        self.species = self.config["species"]
        self.phases = self.config["phases"]
        self.base_phase_structures = self.init_phases()
        self.init_vasp()

    def init_phases(self):
        """
        Initialize the base phase structures.
        """
        workdir = self.root / "structures"
        if not workdir.exists():
            raise IOError(f"Cannot find the directory 'structures' in {str(self.root)}")
        structures = {}
        for phase in self.config["phases"]:
            if phase == "liquid":
                structures[phase] = self.init_liquid_structure()
            else:
                candidates = [x for x in workdir.glob(f"{phase}.*")]
                if len(candidates) == 0:
                    raise ValueError(f"Cannot find the poscar for {phase}")
                if len(candidates) > 1:
                    raise ValueError(f"Multiple poscars for {phase}")
                poscar = candidates[0]
                structures[phase] = read(str(poscar))
        return structures

    def init_liquid_structure(self):
        """
        Initialize the liquid phase structure.

        Num species = 1: use the fcc phase.
        Num species > 1: use SAE (todo)

        """
        if len(self.species) == 1:
            veq = self.config["liquid"]["veq"]
            a = (4 * veq) ** (1 / 3)
            return bulk(self.species[0], crystalstructure="fcc", a=a, cubic=True)
        else:
            raise NotImplementedError(
                "Liquid phase for multi-species is not implemented yet."
            )

    def get_base_structure(self, phase: str) -> Atoms:
        """
        Get the base phase structure.
        """
        return self.base_phase_structures[phase].copy()

    def get_supercells_at_volume(self, phase: str, volume: float) -> Atoms:
        """
        Get the supercell structure at the given atomic volume.
        """
        base = self.get_base_structure(phase)
        scale = (volume / base.get_volume() * len(base)) ** (1 / 3)
        base.set_cell(base.get_cell() * scale, scale_atoms=True)
        supercells = []
        for replicate in self.config[phase]["supercell"]:
            supercells.append(base * replicate)
        return supercells

    def init_vasp(self):
        """
        Initialize the base VASP calculator.
        """
        params = getitem(self.config, ["vasp", "pot"])

        # Setup the POTCARs path
        os.environ["VASP_PP_PATH"] = params["pp_path"]

        # Setup PORCAR for each specie.
        # For ASE VASP calculator, only the suffix is used.
        setups = {}
        if "potcars" in params:
            for i, potcar in enumerate(params["potcars"]):
                if potcar != self.species[i]:
                    setups[self.species[i]] = potcar[len(self.species[i]):]
        else:
            setups["base"] = "recommended"
        xc = params.get("xc", "pbe")

        # The NBANDS for VASP calculations.
        self.vasp_nbands = {}

        # Initialize the VASP calculator for AIMD sampling jobs.
        params = getitem(self.config, ["vasp", "sampling"])
        self.vasp = Vasp(
            xc=xc,
            setups=setups,
            ediff=params.get("ediff", 1e-5),
            lreal=params.get("lreal", "Auto"),
            prec=params.get("prec", "Normal"),
            encut=params.get("encut", 400),
            ismear=params.get("ismear", 1),
            sigma=params.get("sigma", 0.1),
            algo=params.get("algo", "normal"),
            isym=params.get("isym", 0),
            nelmin=params.get("nelmin", 4),
            isif=params.get("isif", 2),
            ibrion=params.get("ibrion", 0),
            nsw=params.get("nsw", 5000),
            potim=params.get("potim", 1),
            nwrite=params.get("nwrite", 1),
            lcharg=params.get("lcharg", False),
            lwave=params.get("lwave", False),
            nblock=params.get("nblock", 1),
            maxmix=params.get("maxmix", 60),
        )
        if "npar" in params:
            self.vasp.set(npar=params["npar"])
        if "kpar" in params:
            self.vasp.set(kpar=params["kpar"])
        if "ncore" in params:
            self.vasp.set(ncore=params["ncore"])
        self.vasp_nbands["sampling"] = params.get("nbands", None)

        # Initialize the VASP calculator for high-precision DFT calculations.
        params = getitem(self.config, ["vasp", "calc"])
        self.prec_vasp = Vasp(
            xc=params.get("xc", "pbe"),
            setups=setups,
            ediff=params.get("ediff", 1e-6),
            lreal=params.get("lreal", False),
            kspacing=params.get("kspacing", 0.2),
            prec=params.get("prec", "Accurate"),
            encut=params.get("encut", 500),
            ismear=params.get("ismear", 1),
            sigma=params.get("sigma", 0.05),
            algo=params.get("algo", "normal"),
            isym=params.get("isym", 0),
            nelmin=params.get("nelmin", 4),
            isif=params.get("isif", 2),
            ibrion=params.get("ibrion", -1),
            nsw=params.get("nsw", 1),
            nwrite=params.get("nwrite", 1),
            lcharg=params.get("lcharg", False),
            lwave=params.get("lwave", False),
            nblock=params.get("nblock", 1),
            maxmix=params.get("maxmix", 60),
        )
        if "npar" in params:
            self.prec_vasp.set(npar=params["npar"])
        if "kpar" in params:
            self.prec_vasp.set(kpar=params["kpar"])
        if "ncore" in params:
            self.prec_vasp.set(ncore=params["ncore"])
        self.vasp_nbands["calc"] = params.get("nbands", None)

    # --------------------------------------------------------------------------
    # Iterators
    # --------------------------------------------------------------------------

    def sampling_task_iterator(self):
        """
        Iterate through all AIMD sampling job dirs.
        """
        workdir = self.root / "sampling"
        return workdir.glob("*/n[pv]t/*/*_*K_to_*K")

    def accurate_dft_calc_iterator(self):
        """
        Iterate through all high precision dft calculation job dirs.
        """
        workdir = self.root / "calc"
        return workdir.glob("*atoms/group*/task*")

    # --------------------------------------------------------------------------
    # Generate VASP AIMD sampling jobs
    # --------------------------------------------------------------------------

    def init_vasp_sampling_parameters(self, npt=False):
        """
        Setup the sampling parameters for VASP.
        """
        params = getitem(self.config, ["vasp", "sampling"])
        gamma = params.get("langevin_gamma", 10)

        # MDALGO = 3 for Langevin thermostat
        # ISIF = 2 for NVT ensemble
        # LANGEVIN_GAMMA: Langevin friction coefficient for atom species
        self.vasp.set(mdalgo=3, langevin_gamma=[gamma] * len(self.species))

        # Special parameters for Parrinello-Rahman NPT.
        # ISIF = 3 for NPT ensemble.
        # LANGEVIN_GAMMA_L: Langevin friction coefficient for lattice degrees of
        # freedom.
        # PMASS (optional tag for VASP) is ignored.
        if npt:
            npt_params = getitem(params, ["npt"])
            gamma = npt_params.get("langevin_gamma_l", 10)
            self.vasp.set(isif=3, langevin_gamma_l=gamma)

    @staticmethod
    def _get_nvt_sampling_task(
        jobdir: Path, t0: float, t1: float, natoms: int, override=False
    ):
        """
        Returns (id, name) for the NVT sampling task.
        If the returned id is -1, the task should be skipped.
        """
        files = [x.name for x in jobdir.glob(f"*_{natoms}atoms_*K_to_*K")]
        key = f"{natoms}atoms_{t0:.0f}K_to_{t1:.0f}K"
        exists = False
        for afile in files:
            if afile.endswith(key):
                exists = True
                break
        if exists and not override:
            return -1, None
        if override:
            newid = len(files)
        else:
            newid = len(files) + 1
        return newid, f"{newid}_{key}"

    @staticmethod
    def _get_temperatures(args: dict, vt_method: str, size: int, npt=False):
        """
        Check the temperature related arguments.
        """
        if "temperatures" in args:
            if "tstart" in args or "tstop" in args:
                raise ValueError("Cannot specify both temperatures and tstart/tstop")
            tarray = asarray_or_eval(args["temperatures"])
            if vt_method == "grid":
                return tarray, tarray
            else:
                if len(tarray) != size:
                    raise ValueError("The length of temperatures should be "
                                     "equal to volumes")
                return tarray, tarray
        else:
            if "tstart" not in args or "tstop" not in args:
                raise ValueError(
                    "Either temperatures or tstart/tstop should be specified")
            if vt_method == "grid":
                raise ValueError("Cannot specify tstart/tstop for grid VT method")
            if isinstance(args["tstart"], str):
                t0 = asarray_or_eval(args["tstart"])
            else:
                t0 = scalar2array(args["tstart"], size, "tstart", "volumes")
            if isinstance(args["tstop"], str):
                t1 = asarray_or_eval(args["tstop"])
            else:
                t1 = scalar2array(args["tstop"], size, "tstop", "volumes")
            if len(t0) != size or len(t1) != size:
                raise ValueError("The length of tstart/tstop should be equal to "
                                 "volumes")
            return t0, t1

    def create_vasp_sampling_nvt_tasks(self, override=False):
        """
        Create VASP Langeven NVT sampling jobs: gamma-only.
        """
        workdir = self.root / "sampling"
        workdir.mkdir(exist_ok=True)
        batch_jobs = []

        # The NVT vasp parameters
        self.init_vasp_sampling_parameters(npt=False)

        for phase in self.phases:
            # The phase sampling parameters
            args = getitem(self.config, ["vasp", "sampling", "nvt", phase])
            if len(args) == 0:
                continue

            # Get the volumes. If the volumes is a string, eval it. 
            volumes = asarray_or_eval(args.get("volumes", []))
            if len(volumes) == 0:
                continue
            size = len(volumes)

            # Get the temperatures. 
            vt_method = args.get("vt_method", "pair")
            t0, t1 = self._get_temperatures(args, vt_method, size)

            # Make (V, T) grid if needed
            if vt_method == "grid":
                _, t0 = np.meshgrid(volumes, t0)
                volumes, t1 = np.meshgrid(volumes, t1)
                volumes = volumes.flatten()
                t0 = t0.flatten()
                t1 = t1.flatten()

            # Get the number of steps
            steps = args.get("nsteps", 5000)

            # Loop through the volumes
            for i, vol in enumerate(volumes):
                jobdir = workdir / phase / f"nvt/v{np.round(vol*100, 0):.0f}"
                jobdir.mkdir(parents=True, exist_ok=True)

                # Get the supercell structures
                supercells = self.get_supercells_at_volume(phase, vol)
                if len(supercells) == 0:
                    continue

                for supercell in supercells:
                    # Determine the task id and name
                    taskid, taskname = self._get_nvt_sampling_task(
                        jobdir, t0[i], t1[i], len(supercell), override
                    )
                    if taskid < 0:
                        continue
                    # setup the Vasp calculator and generate input files.
                    taskdir = jobdir / taskname
                    taskdir.mkdir(exist_ok=True)
                    self.vasp.set(directory=str(taskdir))
                    self.vasp.set(tebeg=t0[i], teend=t1[i], nsw=steps)
                    if self.is_finite_temperature:
                        avg_temp = (t0[i] + t1[i]) / 2
                        self.vasp.set(sigma=avg_temp * kB, ismear=-1)
                    nbands = self.vasp_nbands["sampling"]
                    if nbands is not None:
                        if isinstance(nbands, dict):
                            self.vasp.set(nbands=nbands[str(len(supercell))])
                        else:
                            self.vasp.set(nbands=nbands)
                    self.vasp.write_input(supercell)
                    metadata = {
                        "phase": str(phase),
                        "ensemble": "nvt",
                        "P/V": f"v{vol:.2f}",
                        "Tstart": int(t0[i]),
                        "Tstop": int(t1[i]),
                        "nsw": int(steps),
                    }
                    with open(taskdir / "metadata.json", "w") as fp:
                        json.dump(metadata, fp, indent=2)
                        fp.write("\n")
                    print(f"[VASP/nvt/sampling/gen]: {taskdir}")
                    batch_jobs.append(str(taskdir.relative_to('sampling')))
        with open(workdir / "batch_jobs", "a") as fp:
            fp.write("\n".join(batch_jobs) + "\n")

    @staticmethod
    def _get_npt_sampling_task(
        jobdir: Path, t0: float, t1: float, v0: float, natoms: int, override=False
    ):
        """
        Returns (id, name) for the NVT sampling task.
        If the returned id is -1, the task should be skipped.
        """
        vkey = f"v{np.round(v0*100, 0):.0f}"
        files = [x.name for x in jobdir.glob(f"*_{natoms}atoms_{vkey}_*K_to_*K")]
        key = f"{natoms}atoms_{vkey}_{t0:.0f}K_to_{t1:.0f}K"
        exists = False
        for afile in files:
            if afile.endswith(key):
                exists = True
                break
        if exists and not override:
            return -1, None
        if override:
            newid = len(files)
        else:
            newid = len(files) + 1
        return newid, f"{newid}_{key}"

    def create_vasp_sampling_npt_tasks(self, override=False):
        """
        Create VASP Parrinello-Rahman NPT sampling jobs
        """
        workdir = self.root / "sampling"
        workdir.mkdir(exist_ok=True)
        batch_jobs = []

        # The NPT vasp parameters
        self.setup_vasp_sampling_parameters(npt=True)

        for phase in self.phases:
            # The phase sampling parameters
            args = getitem(self.config, ["vasp", "sampling", "npt", phase])
            if len(args) == 0:
                continue

            # Get desired pressures
            pressures = asarray_or_eval(args.get("pressures", []))
            if len(pressures) == 0:
                continue
            size = len(pressures)

            # Get temperatures
            volumes = scalar2array(asarray_or_eval(args["volumes"]), size, 
                                   n1="volumes", n2="pressures")
            t0 = scalar2array(asarray_or_eval(args["tstart"]), size, 
                              n1="tstart", n2="pressures")
            t1 = scalar2array(asarray_or_eval(args["tstop"]), size, 
                              n1="tstop", n2="pressures")
            steps = args.get("nsteps", 5000)

            # Loop through the pressures
            for i, pressure in enumerate(pressures):
                jobdir = workdir / phase / f"npt/{pressure:.0f}GPa"
                jobdir.mkdir(parents=True, exist_ok=True)

                # Get the supercell structures
                supercells = self.get_supercells_at_volume(phase, volumes[i])
                if len(supercells) == 0:
                    continue

                for supercell in supercells:
                    # Determine the task id and name
                    taskid, taskname = self._get_npt_sampling_task(
                        jobdir, t0[i], t1[i], volumes[i], len(supercell), override
                    )
                    if taskid < 0:
                        continue
                    # setup the Vasp calculator and generate input files.
                    taskdir = jobdir / taskname
                    taskdir.mkdir(exist_ok=True)
                    self.vasp.set(directory=str(taskdir))
                    self.vasp.set(tebeg=t0[i], teend=t1[i], nsw=steps)
                    self.vasp.set(pstress=pressure * 10)
                    if self.is_finite_temperature:
                        avg_temp = (t0[i] + t1[i]) / 2
                        self.vasp.set(sigma=avg_temp * kB)
                    self.vasp.write_input(supercell)
                    metadata = {
                        "phase": str(phase),
                        "ensemble": "npt",
                        "P/V": f"{pressure}GPa",
                        "Tstart": int(t0[i]),
                        "Tstop": int(t1[i]),
                        "nsw": int(steps),
                    }
                    with open(taskdir / "metadata.json", "w") as fp:
                        json.dump(metadata, fp, indent=2)
                        fp.write("\n")
                    print(f"[VASP/npt/sampling/gen]: {taskdir}")
                    batch_jobs.append(str(taskdir.relative_to('sampling')))
        with open(workdir / "batch_jobs", "a") as fp:
            fp.write("\n".join(batch_jobs) + "\n")

    # --------------------------------------------------------------------------
    # Check the status of sampling jobs
    # --------------------------------------------------------------------------

    @staticmethod
    def _is_sampling_job_finished(jobdir: Path):
        """
        Return True if the sampling job is finished.
        """
        metadata = jobdir / "metadata.json"
        if not metadata.exists():
            return False
        with open(metadata, "r") as fp:
            status = json.load(fp)
        if status.get("SU", -1) <= 0:
            return False
        return True

    def update_status_of_sampling_job(self, jobdir: Path):
        """
        Update and return the status of a sampling job at given jobdir.
        """
        metadata = jobdir / "metadata.json"
        if not metadata.exists():
            return {}

        with open(metadata, "r") as fp:
            status = json.load(fp)

        status["nrun"] = -1
        status["device"] = ""
        status["SU"] = -1
        status["processed"] = "n"

        oszicar = jobdir / "OSZICAR"
        if not oszicar.exists():
            return status

        cmd = f"grep '.*T=.*E=.*' {oszicar} | tail -n 1"
        p = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
        stdout, _ = p.communicate()
        if p.returncode != 0:
            return status

        nrun = int(stdout.decode().split()[0])
        status["nrun"] = nrun

        # The simulation is not finished
        if nrun < status["nsw"]:
            return status

        # The simulation is finished
        su = get_vasp_job_service_unit(jobdir / "OUTCAR")
        status["device"] = su.device
        status["SU"] = np.round(su.hours, 2)
        with open(metadata, "w") as fp:
            json.dump(status, fp, indent=2)
            fp.write("\n")

        # The job is already postprocessed.
        output_file = jobdir / "trajectory.extxyz"
        if output_file.exists():
            status["processed"] = "y"

        return status

    def get_status_of_all_sampling_jobs(self):
        """
        Get the status of all sampling jobs.
        """
        status = {}
        for task in self.sampling_task_iterator():
            info = self.update_status_of_sampling_job(task)
            if len(info) == 0:
                continue
            for key, value in info.items():
                status[key] = status.get(key, []) + [
                    value,
                ]
        df = pd.DataFrame(status)
        df.set_index("phase", inplace=True)
        print(df.to_string())
        with open(self.root / "sampling" / "status", "w") as fp:
            fp.write("# " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n")
            df.to_string(fp)

    def list_unsubmitted_sampling_jobs(self):
        """
        List unsubmitted AIMD sampling jobs.
        """
        for task in self.sampling_task_iterator():
            metadata = task / "metadata.json"
            if not metadata.exists():
                print(task)
            status = self.update_status_of_sampling_job(task)
            if status.get("nrun", -1) < 0:
                print(task)

    # --------------------------------------------------------------------------
    # Post-processing
    # --------------------------------------------------------------------------

    def post_process_sampling_job(self, jobdir: Path):
        """
        Post-processing a sampling job.
        """
        # Check if the job is finished
        if not self._is_sampling_job_finished(jobdir):
            return
        # Check if the vasprun.xml file exists
        vasprun_xml = jobdir / "vasprun.xml"
        if not vasprun_xml.exists():
            return
        # Check if the trajectory file exists
        output_file = jobdir / "trajectory.extxyz"
        if output_file.exists():
            return
        # Read the xml file using the api `tensoralloy.io.vasp.read_vasp_xml`
        try:
            trajectory = [
                atoms
                for atoms in read_vasp_xml(
                    vasprun_xml, 
                    index=slice(0, None, 1), 
                    finite_temperature=self.is_finite_temperature
                )
            ]
        except Exception as excp:
            print(f"[VASP/sampling/postprocess]: FAILED to read {vasprun_xml}")
            return
        if len(trajectory) == 0:
            return
        # Add the source information to the atoms.info
        for i, atoms in enumerate(trajectory):
            src = f"{str(jobdir)}@{i}"
            atoms.info["_source"] = src
            atoms.info["_hash"] = hashlib.md5(src.encode()).hexdigest()
        # Save the trajectory to an extxyz file
        write(output_file, trajectory, format="extxyz")
        print(f"[VASP/sampling/postprocess]: {jobdir}")

    def post_process_all_sampling_jobs(self):
        """
        Post-processing all sampling jobs.
        """
        for task in self.sampling_task_iterator():
            self.post_process_sampling_job(task)

    # --------------------------------------------------------------------------
    # High-precision DFT calculations
    # --------------------------------------------------------------------------

    def setup_vasp_accurate_dft_parameters(self, atoms):
        """
        Setup the parameters for high-precision DFT calculations.

        ASE doe not implement the non-collinear settings, so we should hack it.
        """
        params = getitem(self.config, ["vasp", "calc"])
        magmon_orig_value = None
        for key, value in params.items():
            if key == "magmom":
                magmon_orig_value = value
            else:
                self.prec_vasp.set(**{key: value})
        if magmon_orig_value is not None:
            if self.prec_vasp.bool_params["lsorbit"]:
                magmom = f"{len(atoms)*3}*{magmon_orig_value}"
            else:
                magmom = f"{len(atoms)}*{magmon_orig_value}"
        else:
            magmom = None
        if self.is_finite_temperature:
            self.prec_vasp.set(sigma=atoms.info['etemperature'])
            self.prec_vasp.set(ismear=-1)
        nbands = self.vasp_nbands["calc"]
        if nbands is not None:
            if isinstance(nbands, dict):
                self.prec_vasp.set(nbands=nbands[str(len(atoms))])
            else:
                self.prec_vasp.set(nbands=nbands)
        return {"MAGMOM": magmom}

    def create_vasp_accurate_dft_tasks(self, interval=50, shuffle=False):
        """
        Create VASP high precision DFT calculation tasks.
        """
        workdir = self.root / "calc"
        workdir.mkdir(exist_ok=True)

        # The global hash table
        hash_file = workdir / "hash.json"

        # The global training structures file
        calc_file = workdir / "accurate_dft_calc.extxyz"

        # May read existing results
        if hash_file.exists():
            with open(hash_file, "r") as fp:
                hash_table = json.load(fp)
            calc_list = read(calc_file, index=":")
            if len(calc_list) != len(hash_table):
                raise IOError(
                    f"{calc_file}(n={len(calc_list)}) does not "
                    f"match with {hash_file}(n={len(hash_table)})!"
                )
        else:
            hash_table = {}
            calc_list = []

        # Initialize the subsets
        subset_id = Counter()
        for atoms in calc_list:
            subset_id[len(atoms)] += 1

        # Loop through all sampling tasks
        for task in self.sampling_task_iterator():
            # Check if the trajectory file exists
            trajectory = task / "trajectory.extxyz"
            if not trajectory.exists():
                continue
            # Read the trajectory file
            full = read(trajectory, index=slice(0, None, 1))
            if shuffle:
                size = len(full) // interval
                selected = np.random.choice(
                    full[interval - 1 :], size=size, replace=False
                )
            else:
                # Select the last snapshot of each block(size=interval)
                selected = full[interval - 1 :: interval]
            for atoms in selected:
                hash_id = atoms.info["_hash"]
                src = atoms.info["_source"]
                if hash_id in hash_table:
                    continue
                else:
                    calc_list.append(atoms)
                    natoms = len(atoms)
                    aid = f"{natoms}.{subset_id[natoms]}"
                    hash_table[hash_id] = {"aid": aid, "source": src}
                    subset_id[len(atoms)] += 1

        # Save the hash table
        with open(hash_file, "w") as fp:
            json.dump(hash_table, fp, indent=2)
            fp.write("\n")

        # Save the structures
        write(calc_file, calc_list, format="extxyz")

        # Create VASP jobs
        subset_size = Counter()
        for atoms in calc_list:
            # The original structure id
            aid = hash_table[atoms.info["_hash"]]["aid"]

            # For structures of different sizes, we may use different CPU/GPU
            # settings. Hence, 'natoms' is the first metric for makeing subsets.
            natoms = len(atoms)
            subsetdir = workdir / f"{natoms}atoms"
            subsetdir.mkdir(exist_ok=True)
            if natoms not in subset_size:
                subset_size[natoms] = Counter()

            # The group id. Each group contains 100 structures at most.
            sid = int(aid.split(".")[1])
            group_id = sid // 100
            groupdir = subsetdir / f"group{group_id}"
            groupdir.mkdir(exist_ok=True)

            # The task id.
            task_id = sid % 100
            taskdir = groupdir / f"task{task_id}"
            taskdir.mkdir(exist_ok=True)

            # Setup the VASP calculator
            magmom_dct = self.setup_vasp_accurate_dft_parameters(atoms)

            # Write the input files
            self.prec_vasp.set(directory=str(taskdir))
            self.prec_vasp.write_input(atoms)

            # Hack the MAGMOM tag for non-collinear calculations
            if magmom_dct is not None:
                with open(taskdir / "INCAR", "a") as fp:
                    fp.write("\n")
                    for key, value in magmom_dct.items():
                        if value is not None:
                            fp.write(f" {key} = {value}\n")

            subset_size[natoms][group_id] += 1

            # Write the metadata
            metadata = {
                "source": atoms.info["_source"],
                "hash": atoms.info["_hash"],
                "aid": aid,
                "group_id": group_id,
                "task_id": task_id,
            }
            if self.is_finite_temperature:
                metadata["etemperature(K)"] = atoms.info["etemperature"] / kB
            with open(taskdir / "metadata.json", "w") as fp:
                json.dump(metadata, fp, indent=2)
                fp.write("\n")

        for natoms, group_size in subset_size.items():
            for group_id, size in group_size.items():
                print(
                    f"[VASP/calc/create/{natoms}]: " f"group{group_id} ({size} tasks)"
                )

    def get_accurate_dft_calculation_status(self):
        """
        Get the status of high precision dft calculations.
        """

        # Determine the total number of prepared jobs
        hash_file = self.root / "calc" / "hash.json"
        if not hash_file.exists():
            return
        with open(hash_file) as fp:
            hash_table = json.load(fp)
        subset_size = Counter()
        for key, value in hash_table.items():
            aid = value["aid"]
            i, j = [int(x) for x in aid.split(".")]
            subset_size[i] = max(subset_size[i], j + 1)

        status = {
            "group": [],
            "total_jobs": [],
            "completed_jobs": [],
            "converged_jobs": [],
            "CPU(jobs)": [],
            "CPU(hours)": [],
            "GPU(jobs)": [],
            "GPU(hours)": [],
        }
        accumulator = {}

        # Loop through all prepared jobs
        for taskdir in self.accurate_dft_calc_iterator():
            metadata = taskdir / "metadata.json"
            if not metadata.exists():
                continue
            with open(metadata, "r") as fp:
                metadata = json.load(fp)
            sid, aid = [int(x) for x in metadata["aid"].split(".")]
            gid = metadata["group_id"]
            key = (sid, gid)
            if key not in accumulator:
                if (gid + 1) * 100 <= subset_size[sid]:
                    n_total = 100
                else:
                    n_total = aid % 100
                accumulator[key] = Counter(
                    {
                        "CPU(hours)": 0.0,
                        "GPU(hours)": 0.0,
                        "CPU(jobs)": 0,
                        "GPU(jobs)": 0,
                        "n_converged": 0,
                        "n_total": n_total,
                        "completed_tasks": [],
                        "converged_tasks": [],
                    }
                )
            su = get_vasp_job_service_unit(taskdir / "OUTCAR")
            if su is None:
                continue
            converged = check_vasp_job_scf_convergence(taskdir / "OUTCAR")
            if converged:
                accumulator[key]["n_converged"] += 1
                accumulator[key]["converged_tasks"].append(str(taskdir))
            accumulator[key]["n_completed"] += 1
            accumulator[key]["completed_tasks"].append(str(taskdir))
            if su.device == "cpu":
                accumulator[key]["CPU(hours)"] += su.hours
                accumulator[key]["CPU(jobs)"] += 1
            else:
                accumulator[key]["GPU(hours)"] += su.hours
                accumulator[key]["GPU(jobs)"] += 1
            # Update the metadata
            with open(taskdir / "metadata.json", "w") as fp:
                metadata["SU"] = su.__dict__
                metadata["converged"] = converged
                json.dump(metadata, fp, indent=2)
                fp.write("\n")

        # Save the group metadata
        for key in accumulator:
            sid, gid = key
            groupdir = Path(f"calc/{sid}atoms/group{gid}")
            with open(groupdir / "metadata.json", "w") as fp:
                json.dump(accumulator[key], fp, indent=2)
                fp.write("\n")

        # Print the status
        for key, value in accumulator.items():
            status["group"].append(f"{key[0]}.g{key[1]}")
            status["total_jobs"].append(value["n_total"])
            status["converged_jobs"].append(value["n_converged"])
            status["completed_jobs"].append(value["n_completed"])
            status["CPU(jobs)"].append(value["CPU(jobs)"])
            status["GPU(jobs)"].append(value["GPU(jobs)"])
            status["CPU(hours)"].append(np.round(value["CPU(hours)"], 2))
            status["GPU(hours)"].append(np.round(value["GPU(hours)"], 2))
        status["group"].append("overall")
        status["total_jobs"].append(sum(status["total_jobs"]))
        status["CPU(jobs)"].append(sum(status["CPU(jobs)"]))
        status["GPU(jobs)"].append(sum(status["GPU(jobs)"]))
        status["CPU(hours)"].append(sum(status["CPU(hours)"]))
        status["GPU(hours)"].append(sum(status["GPU(hours)"]))
        status["completed_jobs"].append(sum(status["completed_jobs"]))
        status["converged_jobs"].append(sum(status["converged_jobs"]))
        df = pd.DataFrame(status)
        df.set_index("group", inplace=True)
        print(df.to_string())
        with open(self.root / "calc" / "status", "w") as fp:
            fp.write("# " + datetime.now().strftime("%Y-%m-%d %H:%M:%S") + "\n")
            df.to_string(fp)

    # --------------------------------------------------------------------------
    # Make the training dataset
    # --------------------------------------------------------------------------

    def _gather_one(self, taskdir: Path, check_job_status=False):
        """
        Gather one completed high precision DFT calculation.
        """
        if check_job_status:
            su = get_vasp_job_service_unit(taskdir / "OUTCAR")
            if su is None:
                return None
            converged = check_vasp_job_scf_convergence(taskdir / "OUTCAR")
            if not converged:
                return None
        else:
            metadata = taskdir / "metadata.json"
            if not metadata.exists():
                return None
            with open(metadata, "r") as fp:
                metadata = json.load(fp)
            if not metadata.get("converged", False):
                return None
            if not metadata.get("SU", {}):
                return None
            su = ServiceUnit(**metadata["SU"])
            if su.hours <= 0:
                return None
        atoms = next(read_vasp_xml(taskdir / "vasprun.xml", 
                                   finite_temperature=self.is_finite_temperature, 
                                   index=0))
        atoms.info["hash"] = metadata["hash"]
        atoms.info["aid"] = metadata["aid"]
        atoms.info["group_id"] = metadata["group_id"]
        atoms.info["task_id"] = metadata["task_id"]
        return atoms

    def gather(self, output_file=None, nprocs=-2, check_job_status=False):
        """
        Gather all completed high precision DFT calculations into a single file.

        Parameters
        ----------
        output_file : str
            The output file name.
        nprocs : int
            The number of processes to use. Default is -2, which means using all
            available CPUs but one.
        check_job_status : bool
            Check the job status before gathering. Default is False.

        """
        from joblib import Parallel, delayed

        all_tasks = [x for x in self.accurate_dft_calc_iterator()]
        objects = Parallel(n_jobs=nprocs, verbose=1)(
            delayed(self._gather_one)(taskdir, check_job_status)
            for taskdir in all_tasks
        )
        objects = [x for x in objects if x is not None]
        print(f"[VASP/gather]: total {len(objects)} structures gathered")

        if output_file is None:
            date = datetime.now().strftime("%Y%m%d")
            output_dir = self.root / "database"
            output_dir.mkdir(exist_ok=True)
            output_file = output_dir / f"database_N{len(objects)}_{date}.extxyz"
        write(output_file, objects, format="extxyz")

    # --------------------------------------------------------------------------
    # Plot
    # --------------------------------------------------------------------------

    def plot(self, filename: Path, figname=None, rcut=6.0):
        """
        Plot the database file. Currently, four plots are generated:

        1. Energy vs. Volume
        2. Force Norm vs. Volume
        3. Energy vs. Pressure
        4. Force Norm vs. Pressure

        """
        from scipy.stats import gaussian_kde
        from ase.db import connect

        if filename.suffix == ".db":
            db = connect(filename)
            trajectory = []
            for i in range(len(db)):
                atoms = db.get_atoms(id=i + 1, add_additional_information=True)
                trajectory.append(atoms)
        else:
            trajectory = read(filename, index=":")
        energies = []
        forces = []
        fnorms = []
        volumes = []
        stresses = []
        pressures = []
        rdf = {}
        temperatures = []
        for atoms in tqdm.tqdm(trajectory):
            natoms = len(atoms)
            volumes.append(atoms.get_volume() / natoms)
            energies.append(atoms.get_potential_energy() / natoms)
            forces.extend(atoms.get_forces().flatten().tolist())
            fnorms.append(np.sqrt(np.sum(atoms.get_forces() ** 2) / natoms))
            stresses.extend(atoms.get_stress(voigt=True).flatten().tolist())
            pressures.append(atoms.get_stress(voigt=True)[:3].mean() / -GPa)
            if self.is_finite_temperature:
                temperatures.append(atoms.info["etemperature"] / kB)
            species = atoms.get_chemical_symbols()
            ilist, jlist, dlist = neighbor_list("ijd", atoms, cutoff=rcut)
            for i, j, d in zip(ilist, jlist, dlist):
                key = tuple(sorted((species[i], species[j])))
                if key not in rdf:
                    rdf[key] = []
                rdf[key].append(d)

        volumes = np.asarray(volumes)
        pressures = np.asarray(pressures)
        energies = np.asarray(energies)
        forces = np.asarray(forces)
        fnorms = np.asarray(fnorms)
        stresses = np.asarray(stresses)

        for key in rdf:
            rdf[key] = np.asarray(rdf[key])

        _, axes = plt.subplots(2, 3, figsize=(12, 7))
        cmap = matplotlib.colormaps["viridis"]

        ax = axes[0, 0]
        ax.plot(volumes, energies, "o", markersize=2, color=cmap(0.0))
        ax.set_xlabel("Volume ($\AA^3$)")
        ax.set_ylabel("Energy (eV/atom)")

        ax = axes[0, 1]
        ax.plot(volumes, fnorms, "o", markersize=2, color=cmap(0.2))
        ax.set_xlabel("Volume ($\AA^3$)")
        ax.set_ylabel("Force norm (eV/$\AA$)")

        ax = axes[0, 2]
        density = gaussian_kde(volumes)
        x = np.linspace(volumes.min(), volumes.max(), 100)
        fx = density(x)
        ax.plot(x, fx, label="Volume")
        ax.set_xlabel("Volume ($\AA^3$)")
        ax.set_ylabel("Scaled Density")
    
        if self.is_finite_temperature:
            ax = axes[1, 0]
            ax.plot(volumes, temperatures, "o", markersize=2)
            ax.set_xlabel("Volume ($\AA^3$)")
            ax.set_ylabel("Temperature (K)")

            ax = axes[1, 1]
            ax.plot(pressures, temperatures, "o", markersize=2)
            ax.set_xlabel("Pressure (GPa)")
            ax.set_ylabel("Temperature (K)")
        else:
            ax = axes[1, 0]
            ax.plot(pressures, energies, "o", markersize=2, color=cmap(0.6))
            ax.set_xlabel("Pressure (GPa)")
            ax.set_ylabel("Energy (eV/atom)")

            ax = axes[1, 1]
            ax.plot(pressures, fnorms, "o", markersize=2, color=cmap(0.8))
            ax.set_xlabel("Pressure (GPa)")
            ax.set_ylabel("Force norm (eV/$\AA$)")

        ax = axes[1, 2]
        for key in rdf:
            density = gaussian_kde(rdf[key], weights=1.0 / rdf[key])
            x = np.linspace(0, 6, 100)
            fx = density(np.linspace(0, 6, 100))
            ax.plot(x, fx, label=f"{key[0]}-{key[1]}")
        ax.legend()
        ax.set_xlabel("Distance ($\AA$)")
        ax.set_ylabel("Scaled Density")

        plt.tight_layout()

        if figname is None:
            figname = filename.name.replace(filename.suffix, ".png")
        plt.savefig(figname, dpi=150)
        plt.close()


def main():
    root_parser = ArgumentParser()

    # Common arguments
    root_parser.add_argument(
        "-i", "--input", default="config.toml", help="The input file"
    )

    # Subparsers
    sub_parsers = root_parser.add_subparsers(dest="command", help="Sub-commands")

    # The subparser for creating DFT jobs
    parser = sub_parsers.add_parser("create")
    parser.add_argument("task", choices=["sampling", "calc"], 
                        help="The type of the DFT jobs to create.")
    parser.add_argument(
        "--interval", 
        type=int, 
        default=50, 
        help="The interval (in timesteps) for fetching snapshots from AIMD "
             "trajectories. Only valid for 'calc' tasks.")
    parser.add_argument(
        "--nvt", 
        default=False,
        action="store_true", 
        help="Create NVT sampling jobs. Only valid for 'sampling' tasks.")
    parser.add_argument(
        "--npt", 
        default=False,
        action="store_true", 
        help="Create NPT sampling jobs. Only valid for 'sampling' tasks.")

    # The subparser for AIMD sampling
    parser = sub_parsers.add_parser(
        "sampling", help="Commands for creating sampling tasks."
    )
    parser.add_argument(
        "--ensemble",
        choices=["nvt", "npt"],
        nargs="+",
        default=["nvt", "npt"],
        help="The MD ensemble to use, nvt or npt.",
    )

    # The subparser for getting task status
    parser = sub_parsers.add_parser("status", help="Comannds for getting task status.")
    parser.add_argument(
        "type",
        choices=["sampling", "calc", "unsubmitted"],
        help="Choose the type of status: sampling for AIMD sampling jobs, calc for "
             "high precision DFT calculations, unsubmitted for unsubmitted jobs.",
    )

    # The subparser for plotting
    parser = sub_parsers.add_parser("plot", help="")
    parser.add_argument("filename", type=Path, help="The database file to plot.")
    parser.add_argument("--rcut", type=float, default=6.0, help="The cutoff radius.")

    # The subparser for the post-processing
    parser = sub_parsers.add_parser("postprocess")
    
    # The subparser for gathering calculated results
    parser = sub_parsers.add_parser("gather")

    # Parse the arguments
    args = root_parser.parse_args()
    db = TensorDB(args.input)
    
    if args.command == "create":
        if args.task == "sampling":
            if not (args.nvt or args.npt):
                print("Please specify at least one ensemble, e.g., appending "
                      "--nvt or --npt to the command.")
            if args.nvt:
                db.create_vasp_sampling_nvt_tasks()
            if args.npt:
                db.create_vasp_sampling_npt_tasks()
        elif args.task == "calc":
            db.create_vasp_accurate_dft_tasks(interval=args.interval)
    elif args.command == "status":
        if args.type == "sampling":
            db.get_status_of_all_sampling_jobs()
        elif args.type == "calc":
            db.get_accurate_dft_calculation_status()
        elif args.type == "unsubmitted":
            db.list_unsubmitted_sampling_jobs()
    elif args.command == "postprocess":
        db.post_process_all_sampling_jobs()
    elif args.command == "gather":
        db.gather()
    elif args.command == "plot":
        db.plot(filename=args.filename, rcut=args.rcut)


if __name__ == "__main__":
    main()
